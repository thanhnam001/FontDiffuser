{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ocr/miniconda3/envs/namvt17_fontdiffuser/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 48])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from src.modules.content_encoder import ContentEncoderV2\n",
    "\n",
    "class VGGFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGFeatureExtractor, self).__init__()\n",
    "        vgg = models.vgg16()\n",
    "        pretrained = torch.load('weights/vgg16-397923af.pth')\n",
    "        vgg.load_state_dict(pretrained)\n",
    "        self.features = vgg.features\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        features = features.permute(0, 2, 3, 1)  # Rearrange to (batch, height, width, channels)\n",
    "        features = features.view(features.size(0), -1, features.size(3))  # Flatten height and width\n",
    "        return features\n",
    "\n",
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        src: src_len x batch_size x img_channel\n",
    "        outputs: src_len x batch_size x hid_dim \n",
    "        hidden: batch_size x hid_dim\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(src)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        return outputs, hidden\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        hidden: batch_size x hid_dim\n",
    "        encoder_outputs: src_len x batch_size x hid_dim,\n",
    "        outputs: batch_size x src_len\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "  \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return F.softmax(attention, dim = 1)\n",
    "    \n",
    "class GRUDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = Attention(enc_hid_dim, dec_hid_dim)\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        inputs: batch_size\n",
    "        hidden: batch_size x hid_dim\n",
    "        encoder_outputs: src_len x batch_size x hid_dim\n",
    "        \"\"\"\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "        a = a.unsqueeze(1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        assert (output == hidden).all()\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, encoder_hidden, decoder_hidden, img_channel, decoder_embedded, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = GRUEncoder(img_channel, encoder_hidden, decoder_hidden, dropout)\n",
    "        self.decoder = GRUDecoder(vocab_size, decoder_embedded, encoder_hidden, decoder_hidden, dropout)\n",
    "        \n",
    "    def forward_encoder(self, src):       \n",
    "        \"\"\"\n",
    "        src: timestep x batch_size x channel\n",
    "        hidden: batch_size x hid_dim\n",
    "        encoder_outputs: src_len x batch_size x hid_dim\n",
    "        \"\"\"\n",
    "\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "\n",
    "        return (hidden, encoder_outputs)\n",
    "\n",
    "    def forward_decoder(self, tgt, memory):\n",
    "        \"\"\"\n",
    "        tgt: timestep x batch_size \n",
    "        hidden: batch_size x hid_dim\n",
    "        encouder: src_len x batch_size x hid_dim\n",
    "        output: batch_size x 1 x vocab_size\n",
    "        \"\"\"\n",
    "        \n",
    "        tgt = tgt[-1]\n",
    "        hidden, encoder_outputs = memory\n",
    "        output, hidden, _ = self.decoder(tgt, hidden, encoder_outputs)\n",
    "        output = output.unsqueeze(1)\n",
    "        \n",
    "        return output, (hidden, encoder_outputs)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        src: time_step x batch_size\n",
    "        trg: time_step x batch_size\n",
    "        outputs: batch_size x time_step x vocab_size\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        device = src.device\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(device)\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        for t in range(trg_len):\n",
    "            input = trg[t] \n",
    "            output, hidden, _ = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "        outputs = outputs.transpose(0, 1).contiguous()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class OCRModel(nn.Module):\n",
    "    def __init__(self, feature_extractor, seqmodel):\n",
    "        super(OCRModel, self).__init__()\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.seqmodel = seqmodel\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.permute(1,0,2)\n",
    "        output = self.seqmodel(features, target)\n",
    "        return output\n",
    "\n",
    "\n",
    "# vgg_extractor = VGGFeatureExtractor()\n",
    "# inp = torch.rand((1,3,64,256))\n",
    "# out = vgg_extractor(inp)\n",
    "# out.shape # torch.Size([1, 16, 512])\n",
    "\n",
    "# encoder = GRUEncoder(512, 512, 512, 0.1)\n",
    "# inp = torch.rand(16,1,512)\n",
    "# out,hidden = encoder(inp)\n",
    "# out.shape, hidden.shape # (torch.Size([16, 1, 1024]), torch.Size([1, 512]))\n",
    "\n",
    "# decoder = GRUDecoder(48, 256, 256, 256, 0.1)\n",
    "# inp = torch.randint(0,48, size=(1,))\n",
    "# hidden = torch.rand(1, 256)\n",
    "# encoder_out = torch.rand(16,1,512)\n",
    "# out,hidden,_ = decoder(inp, hidden, encoder_out)\n",
    "# out.shape, hidden.shape\n",
    "\n",
    "seqmodel = Seq2Seq(48, 512, 512, 512, 512)\n",
    "inp = torch.rand((16,1, 512))\n",
    "tgt = torch.randint(0,48, size=(16,1))\n",
    "out = seqmodel(inp, tgt)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 1024])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = 512  # VGG feature size\n",
    "encoder_hidden = 512\n",
    "decoder_hidden = 512\n",
    "img_channel = 512\n",
    "decoder_emb = 512\n",
    "vocab_size = 1024 #len('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz')  # Number of classes (characters)\n",
    "num_layers = 1\n",
    "\n",
    "# Instantiate the model\n",
    "vgg_extractor = VGGFeatureExtractor()\n",
    "seqmodel = Seq2Seq(vocab_size, encoder_hidden, decoder_hidden, img_channel, decoder_emb)\n",
    "model = OCRModel(vgg_extractor, seqmodel)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# images = torch.rand(1,3,64,256)\n",
    "# labels = torch.randint(0,48, size=(16,1))\n",
    "# output = model(images, labels)\n",
    "# output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vcs = len('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz')\n",
    "vcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 1024])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_encoder = ContentEncoderV2('ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz',\n",
    "                    in_channels=1024,\n",
    "                    n_heads=8,\n",
    "                    d_head=128,)\n",
    "inp = torch.randint(0, vcs+2, size=(1,16))\n",
    "out = content_encoder(inp)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.criterion import ContentPerceptualLoss, SupConLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# num_epochs = 10\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch in dataloader:\n",
    "#         images = batch['image'].to(device)\n",
    "#         labels = batch['label'].to(device)\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(images)\n",
    "        \n",
    "#         # Compute loss\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#     print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "namvt17_wordstylist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
